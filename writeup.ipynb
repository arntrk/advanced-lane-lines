{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding\n",
    "\n",
    "\n",
    "This project use the following topics to find the lane lines on the road.\n",
    "\n",
    "* Compensate for Camera Lens Distorion\n",
    "    - Compute the camera calibration matrix and distortion coefficients.\n",
    "    - Apply a distortion correction to raw images.\n",
    "* Create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "The images for camera calibration are stored in the folder called `camera_cal`.  The images in `test_images` are for testing your pipeline on single frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comensate for Camera Lens Distorion\n",
    "\n",
    "To be able to correctly get the radius of curvature and massure position inside the lane, we need to calibrate the camera to compansate for lens distorion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the camera calibration matrix and distortion coefficents\n",
    "\n",
    "computing camera calibration is done in the following steps:\n",
    "* prepare object point matrix\n",
    "* convert image to grayscale\n",
    "* finding chessboard corners\n",
    "* append detected corners for each image\n",
    "* [optional] draw corners on the image, for checking for detection\n",
    "* calibrate camera with detected corners from all images\n",
    "\n",
    "All these above steps is done in function `calibrate` in `camera.py`, explained in the code bellow, can also be called from `jupyter notebook` or other python scripts as follows:\n",
    "```python\n",
    "    import camera\n",
    "    \n",
    "    # do calibration on chessboard images\n",
    "    ret, mtx, dist, rvecs, tvecs = camera.calibrate('camera_cal', 9, 6)\n",
    "    \n",
    "    # store all data from calibration\n",
    "    camera.store('camera_calibration_data.p', ret, mtx, dist, rvecs, tvecs)\n",
    "```\n",
    "\n",
    "Calibration can also be executed standalone with `camera.py`, from console as this example:\n",
    "```shell\n",
    "$ python camera.py --ipath camera_cal --numx 9 --numy 6 -o camera_calibration_data.p\n",
    "```  \n",
    "\n",
    "It's also possible to store the results from corner detection, for sanity check purposes, for more info on that use parameter `-h` or `--help` to get more information.\n",
    "\n",
    "#### prepating object point matrix\n",
    "\n",
    "Operation for creating object point matrix:\n",
    "```python\n",
    "    objpnt = np.zeros((ny*nx, 3), np.float32)\n",
    "    objpnt[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
    "```\n",
    "Here `nx` and `ny` correspons number of corners in x and y direction on the chessboard calibration images, given as paramter `--numx` and `--numy` in the command line tool. Look at the number of detected points in x and y direction in the `example of corner finding result` image bellow.\n",
    "\n",
    "#### convert image to grayscale\n",
    "\n",
    "This is done by `cv2.cvtColor` function as follows:\n",
    "```python\n",
    "    # convert image to grayscale \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "```\n",
    "\n",
    "\n",
    "#### finding chessboard corners\n",
    "\n",
    "The process of finding chessboard corners is done as follows:\n",
    "```python\n",
    "    #find chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "```\n",
    "where `nx` and `ny` are number of intersecting corners in `x` and `y` direction, which is 9 and 6 for the example images bellow. \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"2\"><center><H3>example of corner finding result</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/corners_calibration2.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/corners_calibration3.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>calibration2</center></td>\n",
    "    <td><center>calibration3</center></td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "#### append detected corners\n",
    "\n",
    "The detected corners is then added to imgpoints array, for being used later for calibration of camera.\n",
    "\n",
    "```python\n",
    "    # append detected corners \n",
    "    imgpoints.append(corners)\n",
    "    objpoints.append(objpnt)\n",
    "```\n",
    "\n",
    "#### calibrate camera with detected corners from all images\n",
    "\n",
    "When above corner detection and collection of results have been done for all chessboards images, then it is time to calculate the distortion matrix `mtx` and the distortion coefficents `dist` as shown bellow. \n",
    "\n",
    "```python\n",
    "    # gives the ret, mtx, dist, rvecs, tvecs\n",
    "    cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "```\n",
    "\n",
    "The result `mtx` and `dist` can be stored for later use, so that this process does not need to be recalculated every time. Only needes to be recalculate if the camera needs to be changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a distortion correction to raw images\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"3\"><center><H3>camera distortion correction</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./test_images/test1.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/undist_diff_test1.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/undist_test1.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>original</center></td>\n",
    "    <td><center>diff between images</center></td>\n",
    "    <td><center>undistorted</center></td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "The images above shows the original camera image to the left, undistored image to the right, and the difference between the two images is shown in the center. Notes that the there is no difference in the center of the image, which indicates that there is no distortion there.\n",
    "\n",
    "\n",
    "Loading the camera calibration matrix and distortion coeffcients from file, is done as follows.\n",
    "\n",
    "```python\n",
    "    import camera\n",
    "    \n",
    "    # load all data from calibration file\n",
    "    data = camera.load('camera_calibration_data.p')\n",
    "    \n",
    "    # get the mtx and dist from dictonary\n",
    "    mtx = data[\"mtx\"]\n",
    "    dist = data[\"dist\"]\n",
    "```\n",
    "\n",
    "Then the undistorted image can obtained as follows:\n",
    "```python\n",
    "    # obtain the undistored image\n",
    "    undistorted = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "```\n",
    "\n",
    "**Note**: This process is done in the initialization of lane detector class:\n",
    "```python\n",
    "    from pipeline import lane_detector\n",
    "    \n",
    "    # create a new lane detector\n",
    "    detect = lane_detector('camera_calibration_data.p')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a thresholded binary image\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"2\"><center><H3>color binary example images</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/test_images/binary/test1_binary.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/test_images/binary/test2_binary.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>test1</center></td>\n",
    "    <td><center>test2</center></td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "Above images shows the result of creating a threshold image. \n",
    "\n",
    "### applying color transform and gradients\n",
    "\n",
    "I choose the L from LUV color space with color threshold 200-255 (green color), R channel from RGB with color threshold 225-255 (blue color) and apply a sobel magnitude on B channel from LAB color space with threshold 145-200 (red color). The L in LUV were often issue of overfitting, due to noise furhter away from the car, which can be seen for the `test1` image file (see warped image below).\n",
    "\n",
    "The bellow code shows howto apply absolute gradient $|\\partial x|$ in $x$ direction:\n",
    "```python\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "    abs_sobelx = np.absolute(sobelx) \n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "```\n",
    "**Note**: it is important to scale the result to the values in $[0-255]$, which is the range of values of a color elememt of one pixel. Also, note that we only work with one color channel.\n",
    "\n",
    "The magnitude of gradients is taken as $mag = \\sqrt{{\\partial x}^2 + {\\partial y}^2 }$, and scaled back to 8 bit value $[0-255]$ as follows:\n",
    "\n",
    "```python\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    # Calculate the magnitude \n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "```\n",
    "\n",
    "Performing threshold operation on is done as follows:\n",
    "```python\n",
    "    binary = np.zeros_like(channel)\n",
    "    binary[(channel >= low) & (channel <= high)] = 1\n",
    "```\n",
    "where `channel` is image with one color channel, and where `low` and `high` is the low and high values of the threshold.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a perspective transform\n",
    "\n",
    "to rectify binary image (\"birds-eye view\")\n",
    "\n",
    "\n",
    "The perspective transformation is performed in two stages with opencv:\n",
    "* getPerspectiveTransform\n",
    "* warpPerspective\n",
    "\n",
    "The first operation takes in a source (blue) and destination (green) rectangle as parameter, and returns a transformation matrix. The images bellow shows the used source (blue) and destination (green) rectangle, for two of the images.\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"2\"><center><H3>source and destination lines</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/test_images/lines/test1_lines.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/test_images/lines/test2_lines.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>test1</center></td>\n",
    "    <td><center>test2</center></td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The images bellow shows the result after second operation have been applied. The left side images bellow represent the left image above, and the right side images bellow represent the right side image above. The top image bellow on each side is the actual warped image, while the bottom image bellow shows which elements is contributing to the warped result.\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"2\"><center><H3>warped binary and color binary images</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>test1</center></td>\n",
    "    <td><center>test2</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/test_images/warped/test1_warped.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/test_images/warped/test2_warped.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/test_images/color/test1_warped.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/test_images/color/test2_warped.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr> \n",
    "   \n",
    "</table>\n",
    "\n",
    "\n",
    "What can be observed from these images is that the blue component, sobel magnitude of L channel, is more present than we like it to be. The left image is the same image is shown in the distorion correction example,above subsection section marked with 2. So, this is a image with light road pavement, which I have not been able to find sutable adjustment for with help  of gradients and threshold values. \n",
    "\n",
    "The left image `test1` have alot of noise which is due to the L channel in LUV color space, see the green color in `test1` image. Need an adaptive way to change the threshold value of L channel, based brightnes of the color relative to distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect lane pixels and fit to find the lane boundary\n",
    "\n",
    "\n",
    "The detection of lane lines starts with creating a histogram of all non zero pixels in y-direction (image height), for every pixel in x-direction (image width). With this histogram one can find peak values for left and right side of center of the image width, which will give the lane lines closesed to the car (if step 4 have given good results). \n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"2\"><center><H3>lower half of binary togher with histogram</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>test1</center></td>\n",
    "    <td><center>test2</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/test_images/histogram/test1.png\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/test_images/histogram/test2.png\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "When these left and right peaks have been found, the detection algorithm is as follows:\n",
    "* devide detection in parts from bottom up \n",
    "    - define a rectagle as can be seen bellow\n",
    "    - start at the lowest part of the image\n",
    "* identify all none zero pixels inside the rectangle\n",
    "* take the mean of all x values of none zero pixel the rectagle\n",
    "    - to be able to center the next rectagle when moving upwards to top of the image\n",
    "* when all 9 iteration rectagles on each side have been processed\n",
    "    - then we have a collection of pixels inside these rectagles shown bellow\n",
    "    - these pixels will then be used to fit a polynomial curve\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"2\"><center><H3>lower half of binary togher with histogram</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>test1</center></td>\n",
    "    <td><center>test2</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/test_images/output/test1_output.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/test_images/output/test2_output.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "The `np.polyfit` function will produce a second degree polynomial in the form (giving by $a$, $b$ and $c$:\n",
    "$$\n",
    "    x = f(x) = ay^2 + by + c\n",
    "$$\n",
    "by giving use $a$, $b$ and $c$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radius of curvature and vehicle position\n",
    "\n",
    "The [radius of curvature](https://www.intmath.com/applications-differentiation/8-radius-curvature.php) is calculated from the following formula:\n",
    "$$\n",
    "    \\rho  = \\frac{\\left[ 1 + {f^{'}}^2(y)\\right]^{{}^3/_2} }{\\left| f^{''}(y)\\right|}\n",
    "$$\n",
    "\n",
    "where $f(y)$ is a polynomial curve obtained from previous step, defined as:\n",
    "$$\n",
    "    x = f(x) = ay^2 + by + c\n",
    "$$\n",
    "\n",
    "The 1st and 2nd derivative of $f(x)$ is easily obtained as:\n",
    "\n",
    "$$\n",
    "    f^{'}(y) = 2ay + b\\,\\,\\,\\, and \\,\\,\\,\\, f^{''}(y) = 2a\n",
    "$$\n",
    "\n",
    "Then the left and right radius of curvature is obtained as follows:\n",
    "```python\n",
    "    # adjust the y in [0..h] by meter per pixel \n",
    "    y = y_eval * ym_per_pix\n",
    "    \n",
    "    a = left_fit[0]\n",
    "    b = left_fit[1]\n",
    "    # calculating left radius of curvature \n",
    "    left_curverad = ((1 + (2*a*y + b)**2)**1.5) / np.absolute(2*a)\n",
    "    \n",
    "    a = right_fit[0]\n",
    "    b = right_fit[1]\n",
    "    # calculating right radius of curvature\n",
    "    right_curverad = ((1 + (2*a*y + b)**2)**1.5) / np.absolute(2*a)\n",
    "```\n",
    "\n",
    "The vehicle position in the lane is calculated by first taking the diff of left and right detected peak, see histogram images above. This is done as follows:\n",
    "\n",
    "```python\n",
    "    left = left_peak - midpoint\n",
    "    right = right_peak - midpoint\n",
    "    diff = right + left\n",
    "```    \n",
    "The sign of the diff is used to indicate if car is left or right side of the center of lane.  \n",
    "  \n",
    "\n",
    "By using lane size 3.7m and deviding total number of pixels between left and right peak, one can calcluate an approximate distance from lane center by multiplying with absolute diff.\n",
    "```python\n",
    "    # vehicle lane 3.7m us highway - https://en.wikipedia.org/wiki/Lane\n",
    "    lane_size = 3.7\n",
    "    value = np.abs(diff) * (lane_size / (np.abs(left) + np.abs(right)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warp the detected lane boundaries into image space.\n",
    "\n",
    "The `output` from detection of lane boundaries in warped space must be transformed back into image space, and this is done by first calculating the inverse matrix $M^{-1}$ by changing the `src` and `dst` points when using the `getPerspectiveTransform` function. Then the `output` (warped space image) can be used to produce a unwarped (image space image), which can then be used to blend with the undistored image.\n",
    "\n",
    "```python\n",
    "    # only width and height, discard depth if provided\n",
    "    w,h = binary.shape[1::-1]\n",
    "        \n",
    "    # get Minv, the transform matrix\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "        \n",
    "    # returned the warped image\n",
    "    unwarped = cv2.warpPerspective(output, Minv, (w,h), flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # mix undist image and the unwarped 'detection' image\n",
    "    result = cv2.addWeighted(undist, 0.9, unwarped, 0.2, 0)\n",
    "```\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"2\"><center><H3>unwarp detection result and blending with undistorted image</H3></center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>test1</center></td>\n",
    "    <td><center>test2</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"./output_images/test_images/result/test1_result.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "    <td><img src=\"./output_images/test_images/result/test2_result.jpg\" width=\"90%\" height=\"90%\"/></td>\n",
    "  </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
